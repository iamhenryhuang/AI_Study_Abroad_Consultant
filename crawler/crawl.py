import asyncio
import json
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import LLMExtractionStrategy 
from crawl4ai import LLMConfig
from crawl4ai.chunking_strategy import RegexChunking
from crawl4ai.content_filter_strategy import PruningContentFilter
#from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerationStrategy
from pydantic import BaseModel, Field
from typing import Optional, List
import litellm
import os
import sys
from get_website import get_website
from dotenv import load_dotenv
from pathlib import Path

CACHE_FILE = "universities_data.json"
load_dotenv()

if sys.platform == 'win32':
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    
root_dir = str(Path(__file__).resolve().parent.parent)
sys.path.append(root_dir)
from scripts.db.ops import import_json

class BasicAdmissionSchema(BaseModel):
    school_name: str = Field(..., description="å­¸æ ¡çš„å…¨åï¼Œä¾‹å¦‚ï¼šUniversity of Southern California")
    program_name: str = Field(..., description="ç³»æ‰€æˆ–å­¸ä½åç¨±ï¼Œä¾‹å¦‚ï¼šMS in Computer Science")
    degree_level: str = Field(..., description="å­¸ä½ç­‰ç´šï¼Œä¾‹å¦‚ï¼šMaster, PhD, Undergraduate")
    toefl_min: Optional[int] = Field(None, description="æ‰˜ç¦ç¸½åˆ†æœ€ä½è¦æ±‚ï¼Œè‹¥ç„¡å‰‡å¡« null")
    ielts_min: Optional[float] = Field(None, description="é›…æ€ç¸½åˆ†æœ€ä½è¦æ±‚")
    gpa_min: Optional[float] = Field(None, description="æœ€ä½ GPA è¦æ±‚ (4.0 æ¨™åˆ¶)")
    deadline: List[str] = Field(..., description="æ‰€æœ‰ç”³è«‹æˆªæ­¢æ—¥æœŸï¼Œä¾‹å¦‚ï¼š['2025-12-15 (Priority)', '2026-01-15 (Final)']")
    recommendation_letters: Optional[str] = Field(None, description="æ¨è–¦ä¿¡çš„è¦æ±‚æè¿°")
    tuition: Optional[str] = Field(None, description="å­¸è²»è³‡è¨Šæè¿°")

#litellm._turn_on_debug()

async def main():
    
    os.environ["GROQ_API_KEY"] = os.environ.get("GROQ_API_KEY")
    #print(litellm.model_list)

    my_llm_config = LLMConfig(
        provider="groq/llama-3.1-8b-instant",
        #api_token="AIzaSyB-fMfa_NYoJ_locZZIoz6YJexxEAEvXBw"
    )
    # 2. å»ºç«‹ç­–ç•¥ï¼šé—œéµåœ¨æ–¼å‚³å…¥ .model_json_schema()
    strategy = LLMExtractionStrategy(
        llm_config=my_llm_config,
        schema=BasicAdmissionSchema.model_json_schema(), 
        extraction_type="scheme",
        chunking_strategy=RegexChunking(chunk_size=2000, chunk_overlap=200),
        instruction="""
        è«‹å¾ç¶²é ä¸­æå–å…¥å­¸é–€æª»è³‡è¨Šã€‚
        1. å­¸æ ¡åç¨±èˆ‡ç³»æ‰€åç¨±è«‹ä½¿ç”¨è‹±æ–‡ program_name åªèƒ½ç”¨ MS in Computer Scienceã€‚
        2. æˆªæ­¢æ—¥æœŸè«‹åˆ—å‡ºæ‰€æœ‰æ‰¹æ¬¡ã€‚
        3. è‹¥åˆ†æ•¸æˆ– GPA æ²’æåˆ°ï¼Œè«‹å¡« nullã€‚
        4. è«‹ç¢ºä¿è¼¸å‡ºç¬¦åˆ JSON æ ¼å¼ï¼Œä¸¦ä¸”èˆ‡æä¾›çš„ schema ä¸€è‡´ã€‚
        5. è«‹åˆ—å‡ºæ‰€éœ€æ¨è–¦ä¿¡æ ¼å¼ä»¥åŠå­¸è²»
        """
    )

    browser_config = BrowserConfig(headless=True)
    content_filter = PruningContentFilter(threshold=0.48)

    crawler_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        extraction_strategy=strategy,
        word_count_threshold=100,
        only_text=True,
        excluded_tags=['nav', 'footer', 'header'], # æ’é™¤å°èˆªæ¬„å’Œé å°¾
    )

    base = {"school_name",
        "program_name",
        "degree_level",
        "toefl_min",
        "ielts_min",
        "gpa_min",
        "deadline",
        "recommendation_letters",
        "tuition",
        "error"
    }

    cache_data = []
    cache_path = Path(CACHE_FILE)
    if cache_path.exists():
        try:
            cache_data = json.loads(cache_path.read_text(encoding="utf-8"))
            print(f"âœ… å·²è¼‰å…¥å¿«å–ï¼Œç›®å‰å…±æœ‰ {len(cache_data)} ç­†è³‡æ–™ã€‚")
        except json.JSONDecodeError:
            print("âš ï¸ å¿«å–æª”æ¡ˆæ ¼å¼éŒ¯èª¤ï¼Œå°‡é‡æ–°é–‹å§‹ã€‚")

    async with AsyncWebCrawler(config=browser_config) as crawler:
        
        target_list = get_website()

        # å»ºç«‹ä¸€å€‹è‡¨æ™‚è³‡æ–™å¤¾ä¾†æ”¾ç”¢å‡ºçš„ JSON
        os.makedirs("temp_json_data", exist_ok=True) 

        for item in target_list:
            school_name = item.get("school_name", "")
            program_name = item.get("program_name", "")
            degree_level = item.get("degree_level", "")
            deadline = item.get("deadline", [])

            hit = False
            for school in cache_data:
                if school.get("school_name") == school_name and school.get("program_name") == program_name and school.get("degree_level") == degree_level and school.get("deadline") == deadline:
                    print(f"å­¸æ ¡è³‡æ–™å·²å­˜åœ¨")
                    hit = True
                    break

            if hit == False:
                result = await crawler.arun(
                    url=item["official_website"], 
                    config=crawler_config
                )
                #print(result.extracted_content)
                
                if result.success:
                    data = json.loads(result.extracted_content)

                    final_summary = {} 
                    for item in data:
                        for key, value in item.items():
                            
                            if final_summary.get(key) is None and value is not None:
                                final_summary[key] = value
                
                            elif key == 'deadline' and isinstance(value, list):
                                existing = final_summary.get(key, [])
                                final_summary[key] = list(set(existing + value))

                    #unique_output = [final_summary] 
                    
                    standard_data = {
                        "school_id": final_summary.get("school_name", "unknown").lower().replace(" ", "_"),
                        "university": final_summary.get("school_name"),
                        "program": final_summary.get("program_name"),
                        "official_link": item.get("official_website"), # å¾ä½ çš„ target_list æ‹¿
                        "description_for_vector_db": f"Degree: {final_summary.get('degree_level')}. Tuition: {final_summary.get('tuition')}",
                        "requirements": {
                            "toefl": {
                                "min_total": final_summary.get("toefl_min"),
                                "is_required": True if final_summary.get("toefl_min") else False,
                                "notes": ""
                            },
                            "ielts": {
                                "min_total": final_summary.get("ielts_min"),
                                "is_required": True if final_summary.get("ielts_min") else False
                            },
                            "minimum_gpa": final_summary.get("gpa_min"),
                            "recommendation_letters": final_summary.get("recommendation_letters"),
                            "interview_required": False
                        },
                        "deadlines": {
                            "fall_intake": None, # ops.py é æœŸ YYYY-MM-DDï¼Œè‹¥ AI æŠ“çš„æ˜¯å­—ä¸²å°±å…ˆå¡« None æˆ–è™•ç†å®ƒ
                            "spring_intake": " | ".join(final_summary.get("deadline", []))
                        }
                    }

                    if final_summary.get("school_name"):
                        print("\n--- æœ€çµ‚åˆä½µç¸½çµ ---")
                        print(json.dumps(final_summary, indent=2, ensure_ascii=False))

                        # 1. æ›´æ–°å¿«å–æª”æ¡ˆ (universities_data.json)
                        cache_data.append(final_summary)
                        with open(CACHE_FILE, "w", encoding="utf-8") as f:
                            json.dump(cache_data, f, indent=2, ensure_ascii=False)
                        
                        # 2. ç”¢å‡ºçµ¦è³‡æ–™åº«ç”¨çš„ JSON æª”æ¡ˆ (temp_json_data/xxx.json)
                        file_name = f"temp_json_data/{standard_data['school_id']}.json"
                        with open(file_name, "w", encoding="utf-8") as f:
                            json.dump(standard_data, f, indent=2, ensure_ascii=False)
                        
                        print(f"âœ… {final_summary['school_name']} è™•ç†å®Œæˆï¼šå·²æ›´æ–°å¿«å–ä¸¦ç”¢å‡º JSON é å‚™æª”ã€‚")
                
                    await asyncio.sleep(20)

                else:
                    print(f"Error: {result.error_message}")

        print("\n--- æ‰€æœ‰çˆ¬å–ä»»å‹™å®Œæˆï¼Œé–‹å§‹åŒ¯å…¥è³‡æ–™åº« ---")
        try:
            #  import_jsonï¼ŒæŒ‡å‘ä½ å­˜æª”çš„è³‡æ–™å¤¾
            success = import_json(data_dirname="temp_json_data")
            if success:
                print("ğŸš€ è³‡æ–™å·²æˆåŠŸåŒæ­¥è‡³ PostgreSQL è³‡æ–™åº«ï¼")
            else:
                print("âŒ è³‡æ–™åº«åŒ¯å…¥å¤±æ•—ï¼Œè«‹æª¢æŸ¥ db/ops.py çš„å ±éŒ¯è¨Šæ¯ã€‚")
        except Exception as e:
            print(f"åŒ¯å…¥éç¨‹ç™¼ç”Ÿéé æœŸéŒ¯èª¤: {e}")
            

if __name__ == "__main__":
    if sys.platform == 'win32':
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    
    try:
        asyncio.run(main())
    except Exception as e:
        print(f"ç¨‹å¼ç™¼ç”ŸéŒ¯èª¤: {e}")
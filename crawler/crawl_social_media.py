import asyncio
import json
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import LLMExtractionStrategy 
from crawl4ai import LLMConfig
from crawl4ai.chunking_strategy import RegexChunking
from crawl4ai.content_filter_strategy import PruningContentFilter
#from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerationStrategy
from pydantic import BaseModel, Field
from typing import Optional, List
import litellm
import os
import sys

import requests
from get_website import get_website
from dotenv import load_dotenv
from pathlib import Path
from bs4 import BeautifulSoup


#litellm._turn_on_debug()

load_dotenv()
os.environ["GROQ_API_KEY"] = os.environ.get("GROQ_API_KEY")

class ConsolidatedAdmissionPost(BaseModel):
    applicant_id: str
    gpa: Optional[float]
    toefl: Optional[int]
    ielts: Optional[float]
    gre: Optional[str]
    admissions: List[str] = Field(description="éŒ„å–çš„å­¸æ ¡èˆ‡å­¸ä½")
    rejections: List[str] = Field(description="æ‹’çµ•çš„å­¸æ ¡èˆ‡å­¸ä½")
    key_advice: List[str] = Field(description="ä½œè€…æåˆ°çš„æª¢è¨æˆ–å»ºè­°äº‹é …")
    pros: List[str] = Field(description="é€™é–“å­¸æ ¡çš„å„ªé»")
    cons: List[str] = Field(description="é€™é–“å­¸æ ¡çš„ç¼ºé»")

# --- å‚³çµ±æ–¹æ³•ï¼šå¿«é€ŸæŠ“å–é€£çµ ---
def get_ptt_links(keyword, board="studyabroad"):
    url = f"https://www.ptt.cc/bbs/{board}/search?q={keyword}"
    # PTT å¿…å‚™ï¼šé 18 æ­²çš„ Cookie
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
    }
    cookies = {'over18': '1'}
    
    print(f"ğŸ“¡ æ­£åœ¨ç™¼é€å‚³çµ± Request æœå°‹: {keyword}")
    res = requests.get(url, headers=headers, cookies=cookies)
    soup = BeautifulSoup(res.text, 'html.parser')
    
    links = []
    # é€™è£¡å°±æ˜¯å‚³çµ±çˆ¬èŸ²çš„ç²¾é«“ï¼šç²¾æº–åˆ‡ç‰‡
    for entry in soup.select('div.r-ent div.title a'):
        title = entry.text.strip()

        if "CS" not in title and "Computer Science" not in title:
            continue

        links.append(f"https://www.ptt.cc{entry['href']}")
    
    return links

# --- ç¾ä»£ AI æ–¹æ³•ï¼šåˆ†æå…§æ–‡ ---
async def analyze_posts(urls):
    api_key = os.getenv("GROQ_API_KEY")
    if not urls:
        print("æ²’æœ‰æ‰¾åˆ°ä»»ä½•é€£çµã€‚")
        return

    # AI é…ç½®
    strategy = LLMExtractionStrategy(
        llm_config=LLMConfig(provider="groq/llama-3.1-8b-instant"),
        schema=ConsolidatedAdmissionPost.model_json_schema(),
        extraction_type="schema", 
        instruction="""
        è«‹å°‡æ•´ç¯‡ PTT æ–‡ç« å½™æ•´ç‚ºä¸€å€‹ JSON ç‰©ä»¶ã€‚
        1. æå– GPAã€TOEFLã€GRE ç­‰é‡åŒ–æŒ‡æ¨™ã€‚
        2. æ•´ç†éŒ„å–(Admission)èˆ‡æ‹’çµ•(Rejection)çš„æ¸…å–®ã€‚
        3. è«‹æ•´ç†å‡ºä½œè€…çš„å»ºè­°äº‹é …ï¼Œä¸è¦éºæ¼
        4. è«‹æ•´ç†å‡ºé€™é–“å­¸æ ¡çš„å„ªé»å’Œç¼ºé» ä¸¦åˆ—å…¸è¡¨ç¤º
        """
    )

    async with AsyncWebCrawler() as crawler:
        for url in urls[:5]:  
            print(f" AI æ­£åœ¨åˆ†ææ–‡ç« : {url}")
            result = await crawler.arun(
                url=url,
                config=CrawlerRunConfig(extraction_strategy=strategy)
            )
            if result.success:

                print(f"âœ… çµæœ: {result.extracted_content}")
            await asyncio.sleep(5) 
        


# --- åŸ·è¡Œ ---
if __name__ == "__main__":
    # ç¬¬ä¸€æ­¥ï¼šå‚³çµ±æ‰¾ç¶²å€
    target_urls = get_ptt_links("CMU")
    print(f"âœ… æ‰¾åˆ° {len(target_urls)} ç­†ç¶²å€")
    
    # ç¬¬äºŒæ­¥ï¼šéåŒæ­¥ AI åˆ†æ
    asyncio.run(analyze_posts(target_urls))